{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+88fsy4hWIqIoumXUIHpw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themuzzamil/Hackathon/blob/main/TutorAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Installation\n",
        "\n"
      ],
      "metadata": {
        "id": "ypAKjv4Uawcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Installation -->"
      ],
      "metadata": {
        "id": "anzVSy3wajEa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbdP1gVOQy75",
        "outputId": "9f55def5-424d-4a3f-dd82-d39a6468de45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU pypdf\n",
        "!pip install -qU pypdf scikit-learn langchain_community\n",
        "!pip install --quiet langchain langchain-text-splitters langchain_google_genai\n",
        "!pip install --quiet langchain_chroma\n",
        "!pip install --quiet cohere\n",
        "!pip install --upgrade --quiet langchain\n",
        "!pip install --quiet PyPDF2\n",
        "!pip install --quiet google-api-python-client google-auth-httplib2 google-auth-oauthlib PyPDF2 langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "MeB7QPepbbXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "import sqlite3\n",
        "from google.colab import files\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "from langchain.schema import Document\n",
        "from uuid import uuid4\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langchain.schema import Document\n",
        "from pprint import pprint\n",
        "from typing import Any, List, TypedDict\n",
        "from IPython.display import display, Markdown\n",
        "from langchain_core.runnables.config import RunnableConfig\n",
        "from langgraph.store.base import BaseStore\n"
      ],
      "metadata": {
        "id": "oijmdHHsbGJ2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Call"
      ],
      "metadata": {
        "id": "GeNYofSkbjpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "llm = GoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "lu5llYaEQ3SQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "joxxrwvObonC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload pdf and store in vector database"
      ],
      "metadata": {
        "id": "7Bp7nQrvbpq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Database and File Upload Handling\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# SQLite database setup\n",
        "db_path = \"uploaded_files_metadata.db\"\n",
        "\n",
        "# Function to initialize the database and create table if not exists\n",
        "def initialize_db():\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS uploaded_files (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            collection_name TEXT UNIQUE,\n",
        "            file_name TEXT\n",
        "        )\n",
        "    ''')\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# Function to insert file metadata into SQLite\n",
        "def save_metadata(collection_name, file_name):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''\n",
        "        INSERT OR IGNORE INTO uploaded_files (collection_name, file_name)\n",
        "        VALUES (?, ?)\n",
        "    ''', (collection_name, file_name))\n",
        "    conn.commit()\n",
        "\n",
        "# Close the database connection\n",
        "\n",
        "# Function to load metadata from SQLite\n",
        "def load_metadata():\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('SELECT collection_name, file_name FROM uploaded_files')\n",
        "    rows = cursor.fetchall()\n",
        "    cursor = conn.cursor()\n",
        "    conn.close()\n",
        "\n",
        "    return {row[0]: row[1] for row in rows}\n",
        "\n",
        "\n",
        "\n",
        "# Load and split PDF content\n",
        "def load_pdf(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "            return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process PDF to update vector store\n",
        "def process_pdf(file_name, collection_name):\n",
        "    pdf_content = load_pdf(file_name)\n",
        "    if pdf_content:\n",
        "        print(\"PDF content loaded successfully.\")\n",
        "\n",
        "        # Initialize embedding with actual API key\n",
        "        cohere_api_key = userdata.get(\"Embedding_API\")  # Replace with actual key\n",
        "        embedding_function = CohereEmbeddings(\n",
        "            model=\"embed-english-light-v2.0\",\n",
        "            cohere_api_key=cohere_api_key,\n",
        "            user_agent=\"LangChainCohere\"\n",
        "        )\n",
        "\n",
        "        # Initialize Chroma with the collection name\n",
        "        vector_db = Chroma(\n",
        "            collection_name=collection_name,\n",
        "            embedding_function=embedding_function\n",
        "        )\n",
        "\n",
        "        # Split and chunk the PDF content\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
        "        docs = [Document(page_content=pdf_content)]\n",
        "        chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "        # Add chunks to vector DB\n",
        "        vector_db.add_texts([chunk.page_content for chunk in chunks])\n",
        "\n",
        "        # Optional: Print snippets of chunks\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            print(f\"Chunk {idx}: {chunk.page_content[:100]}...\")\n",
        "\n",
        "        print(f\"PDF data from {file_name} updated in vector_db.\")\n",
        "        save_metadata(collection_name, file_name)  # Save metadata in SQLite\n",
        "\n",
        "# Handle file selection or upload\n",
        "def handle_file_selection() -> str:\n",
        "    uploaded_files_metadata = load_metadata()\n",
        "    if uploaded_files_metadata:\n",
        "        print(\"Previously uploaded files:\")\n",
        "        for i, (collection, file_name) in enumerate(uploaded_files_metadata.items()):\n",
        "            print(f\"{i + 1}: {file_name} (Collection: {collection})\")\n",
        "        choice = input(\"Enter the number of the file to use or 'N' to upload a new one: \")\n",
        "\n",
        "        if choice.upper() == 'N':\n",
        "            # Upload a new file\n",
        "            uploaded = files.upload()\n",
        "            if uploaded:\n",
        "                uploaded_file_path = next(iter(uploaded))\n",
        "                print(f\"Uploaded file: {uploaded_file_path}\")\n",
        "\n",
        "                # Create a unique collection for the new file\n",
        "                unique_collection_name = f\"pdf_chunks_{uuid4()}\"\n",
        "                process_pdf(uploaded_file_path, unique_collection_name)\n",
        "                return unique_collection_name  # Return the collection name of the newly uploaded file\n",
        "        else:\n",
        "            # Use an existing collection\n",
        "            chosen_index = int(choice) - 1\n",
        "            chosen_collection = list(uploaded_files_metadata.keys())[chosen_index]\n",
        "            print(f\"Using previously uploaded file: {uploaded_files_metadata[chosen_collection]}\")\n",
        "            return chosen_collection  # Return the collection name of the chosen file\n",
        "    else:\n",
        "        # No collections exist, upload a new file\n",
        "        print(\"No files found in the vector DB. Please upload a new file.\")\n",
        "        uploaded = files.upload()\n",
        "        if uploaded:\n",
        "            uploaded_file_path = next(iter(uploaded))\n",
        "            print(f\"Uploaded file: {uploaded_file_path}\")\n",
        "\n",
        "            # Create a unique collection for the new file\n",
        "            unique_collection_name = f\"pdf_chunks_{uuid4()}\"\n",
        "            process_pdf(uploaded_file_path, unique_collection_name)\n",
        "            return unique_collection_name  # Return the collection name of the newly uploaded file\n",
        "\n",
        "# Initialize the database\n",
        "initialize_db()\n",
        "# Get the collection name based on user input\n",
        "\n",
        "collection_name = handle_file_selection()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "PkJDyirWQ5Pc",
        "outputId": "43f2af1d-2a7c-49b3-a0df-4f2636e6ac94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No files found in the vector DB. Please upload a new file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-73c63dee-7306-4db9-a49f-f09c824d58aa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-73c63dee-7306-4db9-a49f-f09c824d58aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions and Main query"
      ],
      "metadata": {
        "id": "VXj9KBkacB5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StateDocument(TypedDict):\n",
        "    human_input: str\n",
        "    ai_output: str\n",
        "    human_input_2: str\n",
        "    chat: str\n",
        "    decision: str\n",
        "    score: int\n",
        "    reason : str\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "config = {\"configurable\": {\"user_id\": input(\"enter your username\")}}\n",
        "across_thread_memory = InMemoryStore()\n",
        "\n",
        "\n",
        "# Define a VectorDatabase class to interact with the Chroma database\n",
        "class VectorDatabase:\n",
        "    def __init__(self, chroma_db: Chroma):\n",
        "        self.chroma_db = chroma_db\n",
        "\n",
        "    def similarity_search(self, query: str, k: int) -> List[Any]:\n",
        "        \"\"\"Performs a similarity search using the Chroma database.\"\"\"\n",
        "        return self.chroma_db.similarity_search(query, k=k)\n",
        "\n",
        "def questions(state: StateDocument) -> StateDocument:\n",
        "    total = input(\"How many questions you want to generate? \")\n",
        "    state[\"human_input\"] = total\n",
        "    return state\n",
        "\n",
        "def node_0(state: StateDocument) -> StateDocument:\n",
        "    state[\"decision\"] = input(\"IF You Want To Chat Then Type Chat Or Else We Will Continue Towards Quiz: \").lower().strip()\n",
        "    if state[\"decision\"] == \"chat\":\n",
        "        chatbot(vector_db, state)\n",
        "\n",
        "    else:\n",
        "        questions(state)\n",
        "        node_2(vector_db, state)\n",
        "        display_quiz(state)\n",
        "\n",
        "\n",
        "    return state\n",
        "\n",
        "def chatbot(db: VectorDatabase, state: StateDocument) -> StateDocument:\n",
        "    while True:\n",
        "        state[\"human_input_2\"] = input(\"What do you want to know about? (type 'quit' to exit chat): \").strip()\n",
        "\n",
        "        if state[\"human_input_2\"].lower() == \"quit\":\n",
        "            print(\"Exiting chat mode.\")\n",
        "            break\n",
        "\n",
        "        state = node_3(db, state, config, across_thread_memory)\n",
        "        display(Markdown(state[\"chat\"]))\n",
        "\n",
        "    quiz_ch = input(\"do you want to continue quiz yes/no\").lower()\n",
        "    if quiz_ch == \"yes\":\n",
        "        questions(state)\n",
        "        node_2(db, state)\n",
        "        display_quiz(state)\n",
        "    else:\n",
        "        print(\"good bye\")\n",
        "\n",
        "    return state\n",
        "\n",
        "def node_2(db: VectorDatabase, state: StateDocument) -> StateDocument:\n",
        "\n",
        "    question = state[\"human_input\"]\n",
        "    # Format the prompt for quiz creation\n",
        "    prompt = f\"\"\"You are an expert quiz creator. Create {question} quiz questions. Each question should have four multiple-choice options (A, B, C, D),\n",
        "    and provide the correct answer at the end of each question.\n",
        "    Generate a quiz.\n",
        "\n",
        "\n",
        "    Strictly adhere to the following format for each question:\n",
        "\n",
        "    Question: [Question Text]\n",
        "    A. [Option A]\n",
        "    B. [Option B]\n",
        "    C. [Option C]\n",
        "    D. [Option D]\n",
        "    Answer: [Correct Option Letter]  ## Emphasize the letter format\n",
        "\n",
        "    For example:\n",
        "    Question: What is the capital of France?\n",
        "    A. Berlin\n",
        "    B. Madrid\n",
        "    C. Paris\n",
        "    D. Rome\n",
        "    Answer: C\n",
        "\n",
        "    Do not deviate from this format. Do not hallucinate. Provide all questions in this exact pattern.\n",
        "    Don't hallucinate in how many fuction you you are called keep the same format for question.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Retrieve context from the database\n",
        "    relevant_docs = db.chroma_db.get()\n",
        "    documents = [Document(page_content=text) for text in relevant_docs[\"documents\"]]\n",
        "    context = \" \".join([doc.page_content for doc in documents])\n",
        "\n",
        "\n",
        "    prompt_with_context = f\"{prompt}\\n\\nContext: {context},\\n\\nQuestion:{question}\"\n",
        "\n",
        "    # Call your LLM here (replace with actual LLM call)\n",
        "    result = llm.invoke(prompt_with_context,)\n",
        "    state[\"ai_output\"] = result\n",
        "    return state\n",
        "\n",
        "def node_3(db: VectorDatabase, state: StateDocument, config: RunnableConfig, store: BaseStore) -> StateDocument:\n",
        "    # Retrieve context related to recent PDF for chat input\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "    namespace = (user_id)\n",
        "    existing_memory = store.get(namespace, \"user_memory\")\n",
        "\n",
        "    if existing_memory:\n",
        "        existing_memory_content = existing_memory.value.get('memory')\n",
        "    else:\n",
        "        existing_memory_content = \"No existing memory found.\"\n",
        "\n",
        "\n",
        "    # Check if existing_memory is a dictionary and has a 'memory' key.\n",
        "    # If not, assume it's the memory content and use it directly.\n",
        "\n",
        "\n",
        "    relevant_docs = db.similarity_search(state[\"human_input_2\"], k=2)\n",
        "    context = \" \".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "    # Construct the prompt with context\n",
        "    prompt = f\"\"\"\n",
        "    Provide information based on the context below.\n",
        "    Don't hallucinate or web search information.\n",
        "    If user greet you then interact with him greet him ask him how can you help him today.\n",
        "    But when user ask any question then provide him with answer and at end say how can i assist you further\n",
        "    or prompt similar to it just to help out user.\n",
        "    Query input: \"{state['human_input_2']}\"\n",
        "    Context: {context}\n",
        "\n",
        "    You are a helpful assistant with memory that provides information about the user.\n",
        "    If you have memory for this user, use it to personalize your responses.\n",
        "    Here is the memory (it may be empty): {existing_memory_content}\n",
        "    User: {state[\"human_input_2\"]}\"\"\"\n",
        "\n",
        "    # Invoke the LLM with the prompt, no need to format again\n",
        "    state[\"chat\"] = llm.invoke(prompt)\n",
        "    store.put(namespace, \"user_memory\", {\"memory\": state[\"chat\"]})  # Use send_message and extract the text from the response\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cohere_api_key = userdata.get(\"Embedding_API\")  # Replace with actual key\n",
        "embedding_function = CohereEmbeddings(\n",
        "    model=\"embed-english-light-v2.0\",\n",
        "    cohere_api_key=cohere_api_key,\n",
        "    user_agent=\"LangChainCohere\"\n",
        ")\n",
        "\n",
        "def display_quiz(state: StateDocument) -> None:\n",
        "    quiz_content = state[\"ai_output\"].strip().split(\"\\n\\n\")\n",
        "    state[\"score\"] = 0  # Initialize score\n",
        "    questions = []\n",
        "\n",
        "    for content in quiz_content:\n",
        "      lines = content.splitlines()\n",
        "      if len(lines) < 6:\n",
        "          print(f\"Skipping incomplete question block: {content}\")\n",
        "          continue\n",
        "      question = lines[0]\n",
        "      options = lines[1:5]\n",
        "      # Ensure exactly four options; fill missing ones if necessary\n",
        "      while len(options) < 4:\n",
        "          options.append(\"N/A\")  # Placeholder for missing options\n",
        "      correct_answer = lines[5].split(\": \")[-1].strip()\n",
        "      questions.append((question, options, correct_answer))\n",
        "\n",
        "    for idx, (question, options, correct_answer) in enumerate(questions):\n",
        "        display(Markdown(f\"\\n{question}\"))\n",
        "        for option in options:\n",
        "            display(Markdown(option))\n",
        "\n",
        "        answer = input(\"Choose an answer (A/B/C/D): \").strip().upper()\n",
        "        if answer == correct_answer[0]:\n",
        "            state[\"score\"] += 1\n",
        "            print(\"Correct!\")\n",
        "        else:\n",
        "            print(f\"Wrong! The correct answer is: {correct_answer}\")\n",
        "\n",
        "    print(f\"\\nYour final score: {state['score']} out of {len(questions)}\")\n",
        "\n",
        "def reason(db: VectorDatabase, state: StateDocument) -> StateDocument:\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "  You are a helpful assistant. Use the Context:{context} and provide reason why this answer is correct of quiz \"{ai_output}\"\n",
        "\n",
        "  \"\"\") # Removed state and added ai_output directly\n",
        "  relevant_docs = db.similarity_search(state[\"ai_output\"], k=2)\n",
        "  context = \" \".join([doc.page_content for doc in relevant_docs])\n",
        "  result = prompt.format(context=context, ai_output=state[\"ai_output\"]) # Pass ai_output during format call\n",
        "  state[\"reason\"] =  llm.invoke(result)\n",
        "  display(Markdown(state[\"reason\"]))\n",
        "  return state\n",
        "\n",
        "\n",
        "\n",
        "state: StateDocument = {\"human_input\": \"\", \"human_input_2\": \"\", \"decision\": \"\", \"chat\": \"\", \"ai_output\": \"\", \"score\": 0,\"reason\":\"\"}\n",
        "\n",
        "\n",
        "if collection_name:\n",
        "\n",
        "    vector_db = VectorDatabase(Chroma(collection_name=collection_name, embedding_function=embedding_function))\n",
        "state = node_0(state)\n",
        "\n",
        "state = reason(vector_db, state)"
      ],
      "metadata": {
        "id": "6Tj3HFcPRG-q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}